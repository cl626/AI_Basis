* 贝叶斯估计

  

* 决策树与信息熵

  * 信息熵$H(D)=-\sum_{i=1}^n p(X=x_i)log(P(X=x_i))=-\sum p_ilog(p_i)$，信息熵越大，<font color=Red>(种类的)</font>不确定度越大，H(D)=0，样本完全确定
  * 对分类问题，按照信息熵$\to$信息增益比=$\frac 1{H_A(D)}{g(D,A)=\frac 1 {H_A(D)} (H(D)-H(D,A))}$最大化的原则选择特征，逐级下降形成决策树,
  * 据信息熵的有ID3，C4.5 alg
  * ifC(Tson)>C(Tpat)，cut Tson，有CART算法，对regression question，select j,s to minimize $\sum_{x\in R_1(j,s)} (y_i-c_1)^2+\sum_{x\in R_2(j,s)}(y_2-c_2)^2$
  * to sorting problem,based on $Gini_A(D)$<font color=Red>(similar to information entropy)</font>，select A to minimize Gini(D)，剪枝，select best tree
  * <img src="https://s2.loli.net/2023/01/12/HS1AZoBxpDLOgt5.png" alt="image-20230112095816653" style="zoom: 67%;" />

* logistic

  * logistic采用极大似然估计，和最大熵模型$-\sum\widetilde{P}(x)P(y|x)logP(y|x)$(其中$P(y|x)满足E_P(f_i)=E_{\widetilde P}(f_i)$),求$\underset{P\in C}{min}\  \underset{w}{max}L(P,w)$
  * 对偶问题，$\underset{w}{max}\ \underset{P\in C}{min}L(P,w)$
  * 对$\underset{P\in C}{min}L(P,w)$ ，对P(y|x)求导，转化为求$\underset{w}{max}\ \psi(w)$，
  * 这一步用improved iterative scaling求$L(w)=\underset{x,y}{\sum}\widetilde P(x,y)\sum_{i=1}^{n}w_if_i(x,y)-\underset{x}{\sum}\widetilde P(x)logZ_w(x)$关于w的极大值，或用拟牛顿法
  
* SVM

  * 硬间隔支持向量机、软间隔支持向量机、非线性支持向量机（核方法）


* Boost方法——组合权重不同的同一种分类器，得到强分类器

1. Boost与前向分布算法的联系

2. 二分类学习，boost 错误分类的sample weight和误差率低的分类器权重，可用加法模型、损失函数为指数函数、的前向学习算法解释

3. 回归学习提升树，
   $$
   利用前向分布算法f_m(x)=f_{m-1}(x)+T(x;\Theta(m)),\Theta(m)=arg\ \underset{\Theta(m)}{min}(L(y,f_{m-1}(x_i)+\Theta(x_i,\Theta(m))\\
   if\ loss\ function=均方误差损失，\Theta_m={(R_1,s_1),...,(R_j,s_j)}=y-f_{m-1}(x);commonly\ ，由lagrange中值公式，残差用\partial L/\partial f_{m-1}(x)approach
   $$

4. 

* EM——极大似然法的迭代求解(要选好初值点)，正确性与收敛性的证明，求导干极值点，高斯混合模型的期望表示+极大化，期望极大值对应F函数的极大-极大，迭代可以用其他方式，<font color=Red>可用于无监督学习?</font>

* $$
  recessive\ markov——根据隐变量表示出output的最大似然估计P(i,o|\theta)，计算其在P(i,o|\overline\theta)下期望，\\
  拉格朗日乘子法求极大值得\overline\theta，来估计o对应的i，
  $$

* 维比特算法用动态规划求得state 1,2,...,T（近似alg不能保证整体most probably）

* conditional random field——T为高维向量$(X,Y_w)$的随机过程(x,t)

  根据状态特征$s_l(y_i,x,w)$和transfer feature $t_k(y_{i-1},y_i,x,w)$定义条件随机场P(y|x)，可以用前向/back学习算法计算，$P(y_i|x)和P(y_i,y_{i+1}|x)，针对P_w(y|x)的极大似然估计，梯度下降迭代得w，维比特算法得y^*=arg max_{y} P_w(y|x)$



 

